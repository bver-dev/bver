# **Design Document: Bver Property Tax Appeal Service Platform**

## **1\. Overview and Goals**

Bver is a **property tax appeal service platform** designed to streamline and automate the process of challenging property tax assessments. The platform targets a broad residential user base – **individual homeowners, real estate firms with property portfolios, and potentially tax advisors/consultants** – providing each with tools to evaluate and file appeals for residential properties. 

The goal is to make property tax protests more accessible and efficient, given that an estimated *35% of property tax bills contain errors leading to overpayments*, yet only a fraction of owners ever appeal. By offering **real-time assessment of appeal viability**, guided form preparation, and data-driven insights, Bver aims to increase the rate of successful appeals and save users money on over-assessed taxes.

**Key Objectives:**

* **Real-Time Viability Assessment:** Instantly analyze a property’s assessment versus its fair market value and provide feedback on the likelihood of a successful appeal.

* **End-to-End Appeal Workflow:** Enable users to seamlessly go from assessment review to appeal submission within one platform, ideally one click.

* **Multi-Platform Accessibility:** Provide a responsive web application and a mobile app so that users can access the service anywhere. 

* **Focus on Residential MVP:** The initial **MVP** will concentrate on residential property appeals (e.g. single-family homes, condos). This scope allows us to refine the core features with relatively uniform data and simpler use-cases before expanding to more complex property types.

* **User-Friendly Automation:** Replace complex manual processes with automation wherever possible. Many property owners are unfamiliar with appeal procedures; Bver will provide an **“AI-powered” assistant** that simplifies data gathering, analysis and paperwork, lowering the barrier to entry.

* **Significantly Cost-Effective** \- The service aims for a cost of $0 per appeal.

## **2\. Architecture Overview**

Bver’s architecture follows a modular, scalable design that separates concerns between the front end (user interface), back end (business logic and data processing), and external/third-party integrations. Below is a breakdown of the system’s architecture, including data flow, backend and frontend components, and third-party integrations.

### **2.1 System Components and Data Flow**

* **User Interface (Web & Mobile):** Users begin by entering minimal property information into the Bver web or mobile app (e.g. address). They may also upload relevant supporting documents (assessment notice, photos, etc.) or enter additional context. This input is sent to the backend via secure API calls.

* **Real-Time Assessment Module:** The backend’s **Assessment module** receives the user’s property data and immediately fetches supplementary information as needed (from databases or third-party APIs). It then applies the **Automatic Assessment** logic to determine the appeal viability..

* **Data Retrieval & Storage:** If the property’s market value or comparables are needed, the system will query external data sources (see Section 7\) or an internal database. The system can also store user-specific data such as their past appeals, uploaded documents, and generated forms in a database. All sensitive data is stored securely (encrypted at rest and in transit).

* **Appeal Form Generation:** If the user decides to proceed with an appeal, the **Appeal Form Creator** module generates a completed appeal form PDF. The form is the standard template required by the jurisdiction (for example, Georgia uses a standard PT-311A appeal form statewide).   
  Bver’s PDF generator auto-fills the form fields with the user’s information and the assessment details. The generated PDF is then delivered to the user to review, sign ( electronically), and Bver submits this to their local tax assessor.

* **User Submission & Tracking:** In the MVP, the submission process is automated to enhance user convenience and maximize efficiency. Instead of manual user submissions, Bver handles the submission of appeals directly on the user's behalf.

* **Feedback Loop:** After an appeal is submitted and decided, we store the result (approved/rejected and any refund amount). This **feedback data** is valuable for improving the assessment model over time. The architecture will include a mechanism to collect outcomes and user feedback on the process, storing it in the system for future analysis. This will help refine our automatic assessment module (by comparing predicted viability with actual outcomes) and track customer success rates.

### **2.2 Backend Architecture**

The backend is built with a **service-oriented API backend** that handles core logic and communications with databases and external services. Key backend components include:

* **Web Application Server:** Implements the RESTful (or GraphQL) API that the frontends consume. This server contains controllers for various functions – e.g. `AssessmentController` for viability analysis requests, `FormController` for PDF generation requests, and `PropertyController` for retrieving property data. The server can be built with a modern framework (such as Node.js/Express, Python/FastAPI) known for quick iteration and support for data science libraries (Python might be preferred for the assessment logic given potential use of statistical models).

* **Assessment Module (Engine):** Encapsulates the logic for evaluating appeal viability. The assessment module's main responsibility is the decision on the potential appeal, its quantity/numeric/financial value, and its justifications, to the extent that it can meet the requirements of the official appeal form with confidence.  
  This capability is subject to research, but currently, is planned according to the following guidelines:  
  * First, an initial baseline model should be able to reproduce/reverse engineer the county assessment. This is aimed to serve two objectives. One is to increase confidence in the overall pipeline \- correctness of input data, proper feature engineering, and a sanity check on the inference methods. The other objective is to serve as a baseline for the downstream advanced modelling and required appeal justifications \- a starting point/educated guess on the question: what features and info are not used by the county?  
  * Then, an advanced model is aimed to be more personalized for the analyzed property with additional info over the county assessment and over our proprietary baseline model (which ideally should reproduce the county’s assessment with good accuracy). For an appeal to be created, the following conditions should be met:  
    * Our advanced assessment is smaller than the county’s assessment with a significant enough threshold margin.  
    * Our baseline model reproduces with accuracy the county’s assessment.  
    * Our advanced assessment is explainable: the difference(s) can be attributed quantitatively to additional information that we have obtained and used and can describe concisely as a justification in the appeal form.  
  *  Modeling should be probabilistic (rather than just return a deterministic, single value) and return confidence intervals on the predicted values. The conditions for appeal candidates should contain confidence measures and these should be evaluated on historical data as we gradually accumulate it, in probabilistic forecasting tools (for example, metrics such as reliability, probabilistic forecast calibration, etc.).   
  * The modeling pipeline should smoothly facilitate a “fast and easy path” for justification from the user, if they wish to take an active part and upload it. Example: a user knows in advance, coming into our process, that they have a photo of a specific maintenance issue they wish to use. Our interface should allow that, and this should propagate all the way to our assessment modeling and with the correct impact, eventually finding its way into the numeric assessment as well as the properly processed justifications in the appeal form. Same should work for some textual information if the user wishes to actively contribute it.   
  * The above guidelines naturally suggest a method to approach new geography/county/market: a review of available data sources and providers, then reverse engineering of the local county’s assessment (which is a measurable process that has ground truth for significant historical data\!) as the baseline task, and finally the development of an explainable advanced model, once we stand on solid ground. 

* **PDF Form Generator:** A service (or library) responsible for populating official appeal form PDFs. We will maintain a template for each of the **standard appeal forms (SF)** in the MVP’s target jurisdictions. This component takes inputs like owner name, address, parcel ID, assessed value, proposed value, and reasons for appeal, and then produces a PDF. Technologies such as PDFtk, iText, or report generation libraries can be used to fill form fields. 

* **Database:** A relational database (e.g. PostgreSQL) stores persistent data: user accounts and credentials, property records (for cached data or user-added info), appeal case records (each appeal’s details and status), and any content users upload (possibly stored in a cloud storage bucket and referenced in DB, depending on size). The **data schema** for property records will be defined based on the data points needed for assessment – e.g. fields for assessed value, last sale price, lot size, building square footage, year built, location (county), etc.. 

* **Integration Layer:** The backend will also contain integration code to interface with third-party services (detailed in Section 7). For example, a module to call a **property data API** (like Zillow or ATTOM) given an address, or a module to query a public records database. This integration layer abstracts external APIs behind a common interface.

* **Security & Compliance Services:** Responsible for authentication, authorization, and compliance checks. Users will log in (using email/password or SSO options) – an **Auth service** (using JWT tokens or session management) ensures data is protected. Compliance logic can be embedded here (see Section 8\) – e.g. if a user tries to initiate an appeal for a state that is past the deadline, the system can warn or prevent proceeding. Logging of key actions (user logins, form generations) is enabled for audit trails.

The backend will be deployed on a scalable cloud infrastructure (see Deployment) and designed as a **stateless service** (such that it can be horizontally scaled by adding more instances behind a load balancer). This allows Bver to handle increasing load as the user base grows. 

**2.3 Frontend (Web & Mobile)**

The platform’s frontend consists of:

* **Mobile Application:** A dedicated mobile app for both iOS and Android, likely built using a cross-platform framework such as **Flutter** to maximize code sharing with the web where possible. The mobile app offers the same core functionality – property lookup, instant viability feedback, and appeal form generation – with a UI optimized for different form factor screens and touch input.

* **Web Application:** A modern single-page application (SPA) built with a framework like React to support multiple form factors. This provides a dynamic user experience on desktop and mobile browsers. The web app communicates with the backend via REST/GraphQL calls, handling tasks like form input, displaying results, and guiding the user through the workflow. Key UI views include: an onboarding page (a.k.a landing page), a dashboard (showing any properties or appeals the user is tracking), a property input/assessment page (where real-time feedback is given), and an appeal form review page. The web UI is designed to be responsive, so it works on various screen sizes (which also covers users who prefer mobile web over installing an app).

* **Real-Time Feedback UI:** A standout aspect of the UI/UX design is the presentation of **real-time assessment results** as the user inputs data. For instance, as soon as a user enters their property’s address or assessment figures, the app can display a loading indicator and zoom in on the property on the map while the backend computes the viability, then show a **visual indicator of appeal strength**. UX considerations here include using clear graphics or messages (e.g., a color-coded meter or a simple message like “Likely Over-Assessed – Potential Savings: \~$1,200” if an appeal looks promising). This immediate feedback loop is designed to keep users engaged and informed, which is known to improve user satisfaction and trust in the service. Real-time responsiveness in the UI will be achieved by making asynchronous calls to the backend and updating the interface without page reloads, giving a smooth app-like experience on web as well.

* **\[P2, likely post MVP\] Multi-Property Management UI:** Since Bver targets power users like tax advisors and real estate firms (who may handle dozens of properties), the interface will include features for managing multiple properties and appeals. In the MVP, this could be as simple as allowing users to add multiple properties to their account and view a list or dashboard. The design will ensure it’s easy to “bulk add” properties (perhaps via CSV upload in a later update) and to see summary statuses at a glance. For now, adding 20–50 properties manually is supported – the UI will allow sequential entry or editing of each property’s info. This sets the stage for future enhancements like batch processing.

* **Notifications and Alerts:** The UI will incorporate notifications to enhance user interaction. For example, alert banners if an appeal deadline is approaching or if new data (like an updated AVM value) becomes available. Real-time push notifications on mobile (using services like Firebase Cloud Messaging) and email alerts for key events (e.g., a reminder: “Your appeal deadline is in 10 days” or “Your appeal form is ready for submission”). The architecture supports this via backend-triggered events after viability checks or as scheduled reminders.

Overall, the frontend is built for **clarity and ease of use**, following modern UX best practices. It will undergo usability testing to ensure that non-technical users (like many homeowners) can easily navigate the process

**2.4 Third-Party Integration Overview**

Integrations are crucial for Bver to provide accurate data and a seamless workflow. The architecture is designed to integrate with external services in a modular way: each integration is handled by a specific service/class that translates external data into our internal format and stores relevant data in the database for caching, AI training (if found applicable) and future cost reduction (API/service cost). Key integration points include:

* **Property Data APIs:** To fetch property details, valuations, and comparable sales data in real time. For MVP, we plan to integrate with readily available services. For example, the **Zillow API** can provide home valuation data (Zestimates), comparables, and property details for millions of U.S. homes. Zillow’s API is free with certain usage limits and could serve as a quick way to get market value estimates. Another option is the **RentCast (RealtyMole) API**, which offers property records including tax assessment history and AVM-based value estimates, plus comps, with a free tier for low-volume use. These APIs will be called by the backend when the user requests an assessment, if the data is not already in our database. The system will handle API keys and rate limits, and gracefully degrade (e.g., if an API fails, perhaps use a backup source or inform the user to manually input an estimated value).

* **Public Records and Databases:** Where possible, Bver will connect to public data sources. Many county assessors provide online databases of property assessments; a few even offer open data APIs (for instance, the City of Albany provides a public API for property tax assessment data). In the short term, our strategy might involve using **data aggregators** like **ATTOM Data**. ATTOM’s property data API aggregates county assessor records, sales data, ownership info, and even provides its own AVM valuations. ATTOM covers 155 million properties (99% of the US) and could be a one-stop source for both current assessment values and market comps, though it is a paid service (pro tier is at \~3$ per address, likely enterprise tier can reduce this cost). In the MVP, given budget considerations, we might rely on a combination of **free sources** (Zillow, public records) and a **manually curated dataset** (our initial table of 50–100 properties with known values). As we scale, transitioning to robust APIs like ATTOM or others will ensure data coverage nationwide.

* **Mapping and Geocoding Services:** To improve user experience in entering property information, we can integrate an address auto-complete and geocoding service (such as Google Maps API or Mapbox). This would let users easily select their property address and ensure accuracy in location data (latitude/longitude, county identification, etc.). Additionally, geocoding the address helps us route the appeal to the correct jurisdiction and pull the correct forms/deadlines for that county.

* **Document Management and E-Signatures:** The platform may integrate with a service like DocuSign or Adobe Sign in later phases so that users (especially remote ones) can electronically sign the generated appeal PDF. We will consider cloud storage integrations for saving documents – for example, using AWS S3 for storing copies of generated PDFs or any evidence files users upload.

* **Notification Services:** For sending out emails or SMS to users (alerts about deadlines, confirmation of form generation, etc.), integration with a service like SendGrid/Resend (email) or Twilio (SMS) is planned. Push notifications to the mobile apps are a need as well.

* **Analytics and Monitoring:** Though not directly user-facing, we will integrate with analytics platforms (Google Analytics for web usage, or mobile analytics SDKs) to monitor user engagement and identify drop-off points in the workflow. This data will inform UX improvements and experimentations. Also, error monitoring services (like Sentry) will be used to catch and log any runtime issues in both frontend and backend.  
* Should we add dashboard ?

All third-party integrations are **abstracted** so that we can swap providers if needed (for example, if we start with Zillow data and later move to a proprietary dataset). API keys and credentials for external services will be managed securely (not hard-coded, but stored in configuration with proper encryption). The architecture ensures that if an integration faces downtime or changes, it impacts the user minimally – for instance, by having fallback logic or caching last known data.

## **3\. MVP Scope and Technical Specifications**

The MVP for Bver will implement the core functionality for the residential market: residential property tax appeals with real-time guidance. Below are the key features and technical specifics included in MVP (and equally important, what is **out of scope** for MVP). This ensures a focused initial product that can be delivered quickly and reliably, while laying the groundwork for future enhancements.

### **3.1 In-Scope Features (MVP)**

* **Real-Time Appeal Viability Assessment:** Users can input basic property information (at minimum: property address or parcel number, current assessed value, and possibly recent purchase price or their estimate of market value). Upon input, the system provides an instant analysis of whether a tax protest is likely justified. This is powered by the **Auto Assessment module**. The real-time aspect means the system must complete data fetch and computations within a couple of seconds for a smooth UX. Example output: “**Estimated overassessment: $50,000 (15%). Likelihood of successful appeal: High**.” This feature gives users immediate feedback, helping them decide next steps.

* **Appeal Case Management:** The platform will allow users to create and manage appeals. For an individual, this might be just one property; for a tax consultant or multi-home owners, they might handle multiple appeals. MVP supports multiple properties entered by a single user. This is facilitated through a simple dashboard or list interface. Each property/appeal record can store the property details, the viability result, and track whether the user has proceeded to generate a form or submitted the appeal. 

* **Automatic Assessment Module:** The initial version of the assessment module is implemented fully. It encapsulates **domain expert know-how** (gleaned from property tax consultants and reference materials) and **data insights** from our preliminary dataset/literature and builds on top of it. For example: comparing the assessed value to automated market value (AVM) – if assessed \> AVM by a certain percentage, mark viable; or checking for data anomalies (like if the assessor’s recorded square footage is higher than actual by X%). We will incorporate any publicly known strategies for appeals (such as those mentioned in professional guides) into this module.  
  * **Advanced AI/ML:** The initial assessment logic is relatively simple. MVP will also include training of machine learning models, or smart utilization of generic LLMs while remaining LLM agnostic by design.

* Automatic Assessment Module: within the MVP scope, the requirements from the assessment module:  
  * **Baseline model reproduces county assessment with good quality** (quality metrics to be defined, as well as the ground truth to measure against) for at least one county.  
  * **Advanced model utilizes additional info over the baseline model and incorporates at least two new features/data source types** that we are convinced are not used in the county assessment. This confidence should rely on either explicit, direct knowledge from a domain expert/professional assessor, or on our success to reverse engineer the county assessment, and reproduce it with our own feature set, with good accuracy (ideally both).   
  * Evaluation data and ground truth for the consequent prediction tasks should rely on county assessments, and, if available, historical appeals and their resolutions.   
      
* **Automatic Appeal Form Generation (Standard Form PDF):** The MVP will support generating the completed **Standard Appeal Form** (abbreviated SF PDF) automatically. This refers to the official form required to file an appeal in the MVP’s chosen jurisdiction (for example, many counties use a standard form that captures property ID, owner info, reason for appeal, etc.). The MVP will handle one form format (likely for one state/county); support for multiple jurisdictions’ forms will come later, but the system will be easily extendible.

* **Data Extraction Module:** To power the above features, MVP includes a **data extraction/ingestion component**. We will gather data for on the order of 50–100 residential properties (likely in the target market) to build an initial dataset and experiment on (“להתגלח”). This dataset could include information like assessed values vs. recent sale prices or Zillow values, etc. The purpose is twofold: 1\) to aid in developing and testing the assessment algorithm (we can compare its outputs to known outcomes or expert opinions on those cases), and 2\) to possibly serve real user queries if they pertain to those properties. The possibility of getting real successful protest cases can be leveraged here as we can fetch relevant data for that time and compare the result we produce to the successful use case.

* **User Accounts and Roles:** The MVP will have a simple user registration/login system so that each user’s data is private and persisted. Login with Google will also be supported. Fine-grained roles/permissions (like team collaboration) will be minimal in MVP but planned for later.

* **Real-Time Feedback UI:** As described, the front end will visibly update the user with assessment results without needing a page refresh. This will likely use AJAX calls or WebSocket if needed (though standard async calls suffice for a quick result). Ensuring the UI is intuitive is part of MVP’s UX scope – e.g., using clear language to explain the result and next steps (“We recommend filing an appeal. Click below to generate your appeal form and we will submit it for you.” or, “Your assessment seems fair – an appeal may not reduce your taxes significantly.”).

* **Payment Processing and Fees:** Bver’s business model might involve charging users (e.g., a success fee). The MVP, will support integration of payment gateways (Stripe, etc.) or automated invoicing.

* **Testing & Validation (MVP-level):** While this is covered in the Testing section, it’s worth noting in scope that MVP will include internal validation of the assessment module. For example, we will **compare the module’s recommendations with a human expert’s judgement or with known appeal outcomes** for our sample data. The MVP’s success criterion is not just that it functions, but that it provides reasonably accurate advice. We will construct evaluation metrics (like precision of recommending appeals) during MVP development.

### **3.2 Out-of-Scope (MVP) and Limitations**

To manage scope, certain features are **excluded from the MVP** but are noted for future consideration:

* **Commercial or Industrial Properties:** The complexities of commercial property valuations (income approach, etc.) are not handled in MVP. The focus is on residential. Appeals for large commercial properties often require different evidence and possibly legal representation, so those will be considered in later phases.

* **Robust Multi-Tenancy & Team Collaboration:** While advisors can use the MVP, features like multi-user teams, assigning tasks to colleagues, or elaborate client management (CRM features) are deferred. MVP provides single-user login accounts. Collaboration features (sharing access to a property case, messaging within the platform, etc.) come later.

* **Extensive Jurisdiction Customization:** Aside from the standard form and basic deadline checks, MVP will not encode the specific rules of every county/state (like differing appeal windows or evidence requirements). We will likely implement one baseline process that fits a specific target area. As we expand, the platform will need to handle jurisdiction-specific logic (for example, Texas requires filing by May 15 or within 30 days of notice, some places have fees to file, etc.), but these will be gradually added. In MVP, any jurisdiction-specific compliance needed for our test region can be hard-coded (like simply informing the user of the deadline).

By clearly delineating what MVP entails, we ensure development efforts are focused. **Success criteria for MVP** include: a working web and mobile interface, at least 20 test users (e.g. friendly users or internal testers) successfully evaluating properties and generating appeal PDFs, and positive feedback that the real-time guidance is helpful. With MVP delivered, we will then iterate and expand functionality as outlined next.

As for the assessment module, success criteria for the MVP are:

* Confidence that we have the ability to reproduce the selected county’s assessment with a proprietary baseline model for residential properties, and:  
* A working advanced model that admits at least two additional features/data sources that to the best of our analysis/reverse engineering are not used by the county, and show a finer, more personalized assessment.   
* All modeling results and assessments are explainable (feature importance, and downstream natural language justifications, appeal form facing).

## **4\. UX/UI Considerations for Real-Time Feedback**

Providing an excellent user experience is a top priority, especially given the potentially confusing nature of tax appeals. The UI/UX design will emphasize **clarity, responsiveness, and guidance**, with a special focus on real-time feedback that keep users informed instantly. Below are key considerations and decisions for Bver’s UX/UI:

* **Immediate Visual Feedback:** As soon as a user inputs or updates key data (like changing their property’s estimated value), the interface should update the viability assessment. This will be achieved with dynamic elements such as:

  * A **color-coded indicator or score**: e.g., a gauge or meter that swings from red (low chance) to green (high chance) based on the assessment result. This gives a quick at-a-glance understanding. We’ll supplement color with icons (thumbs-up/down or checkmark/exclamation) and text for accessibility.

  * **Textual explanation**: Alongside the score, a short text like “Over-assessed by \~15%, an appeal is recommended” or “Assessment is in line with market value, appeal not likely to succeed” provides context. It’s important we phrase these in non-technical language for lay users, while still being precise.

  * **Potential Savings Display:** Many users will be motivated by how much money they could save. So the UI will display an estimated tax savings (e.g., “If successful, you may save approximately $X on your tax bill”). This is derived from the difference in assessed value we anticipate and the local tax rate. It personalizes the benefit of appealing.

  * **Loading Indicators:** If any computation or data fetch takes more than a half-second, a spinner or progress bar will show to assure the user that the system is working. This prevents confusion or duplicate submissions. The goal is to keep the real-time feedback feeling instantaneous, but when it isn’t, communicate clearly that the analysis is running.

* **Guided Workflow and Instructions:** Tax appeals can be procedural, so the UI will guide users step by step. At each step, context-sensitive help will be available. For instance, a small “i” or “?” info icon next to “Assessed Value” can explain “This is the value the county has assigned to your property (found on your notice). Enter it here to compare with market value.” Similarly, when showing the viability result, a “How was this calculated?” link can pop up an explanation of factors considered, lending transparency. This builds trust – users know the recommendation isn’t arbitrary but based on data points.

* **Mobile UI Optimization:** On mobile, screen space is limited, so the design will use collapsible sections and concise displays. The real-time feedback might be shown as a simple card or banner on mobile that updates with the result. Interactive elements (buttons, forms) will be sized appropriately for touch. We will ensure that the core actions (like “Generate PDF”) are prominently accessible without excessive scrolling. Mobile may also leverage unique UI elements – e.g., the option to use the camera to scan a document or the phone’s GPS to confirm property location if relevant – to streamline input.

* **Multiple User Types Accommodation:** The interface should be intuitive for novices yet efficient for power users. For an individual homeowner, the app might be the first time they’ve seen terms like “assessed value” or “millage rate”. So we avoid jargon or at least define it. For example, instead of a raw term, we might say “Assessed Value (the value the tax office says your home is worth)”. We will likely include a **glossary or FAQ section** accessible from the UI for common questions. Conversely, a tax advisor might find too much hand-holding tedious, so we might allow parts of the UI to be streamlined (maybe an “advanced mode” toggle or simply designing the flow so it’s naturally quick to bypass explanations for those who don’t need them). Bulk actions (in later phases) like uploading multiple properties or copying data from one to another will be UI considerations for those pro users.

* **UX for Manual vs Automatic Steps:** During MVP, some steps are manual (possibly the final submission to the county). The UI should clearly delineate what Bver will do automatically and what the user is expected to do. For example, after generating the PDF, a screen might say: “**What’s Next?** 1\. Download and sign your appeal form. 2\. Submit it to the County Assessor by mail or in person. (See instructions on the form.) 3\. Update this app with the outcome.” We might provide a checklist that the user can mark off (“Form signed”, “Form sent on date X”) to give a sense of progress and to reinforce the steps. Eventually, if we integrate submissions, this part of the UI will evolve, but for now clarity is key to avoid any user thinking we filed it for them when we haven’t.

* **Consistency and Branding:** The platform should have a clean, professional aesthetic that conveys trust (important for financial/legal related software) but also approachability. We’ll use a consistent color scheme and typography across web and mobile. Interactive elements (like the real-time feedback meter) should look and behave similarly on both platforms. Small animations (such as the meter animating to a value, or a success checkmark when a form is generated) can add polish and make the experience more engaging. However, we’ll be cautious not to distract – the focus is on *function*.

* **Error Handling & Edge Cases in UI:** The design accounts for cases like: what if an address is not found in our data sources? The UI can prompt “We couldn’t automatically find data for your property. Please verify the address or enter details manually.” Another edge case: appeal not eligible (e.g. the deadline passed). The UI should immediately warn if, say, today’s date is after the typical filing window for that county: “The appeal deadline for 2025 was May 15th, 2025\. You may need to wait for next year to file an appeal.” This saves user frustration. If the system encounters any external API errors (e.g., Zillow API down), we will show a friendly message: “Having trouble fetching latest market data – we will update the appeal as soon as the data becomes available.” Maintaining the user’s trust means being transparent about issues and offering alternative actions, rather than just failing silently.

* **Accessibility:** We will strive to make the platform accessible (following WCAG guidelines). This includes proper label tags for form inputs, the ability to navigate via keyboard, screen-reader compatibility, and using high-contrast mode if needed. Real-time feedback should not rely on color alone (use text or icons as well) to be usable by color-blind users. This not only widens our audience but also often improves overall UX quality.

* **User Testing and Iteration:** As part of design consideration, we plan to conduct usability tests during MVP development – with a few target users (perhaps a homeowner, a realtor, a tax consultant) going through the prototype. Their feedback on what is confusing or what they expected versus what happened will be used to refine the interface before full launch. We anticipate iterating the wording of messages and the layout of screens based on such feedback. For example, if users indicate they’re not sure what to do after seeing the result, we’ll adjust the UI to have a clearer call-to-action (like a big “Proceed to Appeal” button if positive, or “Re-check another property” if not).

In summary, the UX/UI is crafted to make a complex, bureaucratic process feel as simple as an online tax calculator. Real-time feedback loops keep the user engaged and informed, reducing anxiety about the unknown. By providing transparency (how we got the result) and guidance (what to do with it), Bver’s interface will build user confidence in taking action on their property tax bills.

## **5\. Deployment and Testing Plans**

A robust deployment strategy and thorough testing are essential to ensure Bver runs reliably and securely, especially as it deals with important financial data. Here we outline how the system will be deployed and the testing regimen at various stages of development.

### **5.1 Deployment Strategy**

**Infrastructure & Environment:** For MVP, we will utilize a cloud-based deployment to allow easy scaling and maintenance. Likely, we’ll choose a major cloud provider (AWS, Google Cloud). The backend API and web application can initially be deployed on a **Platform-as-a-Service** offering – for example, AWS Elastic Beanstalk or Heroku for simplicity – which handles a lot of the underlying server management. This allows quick updates and rollbacks. The database will use a managed cloud database service (e.g., supabase for PostgreSQL) for reliability (backups, redundancy). The mobile app, since it’s client-side, will be distributed via app stores (Apple App Store, Google Play) once ready.

**Containerization:** We will containerize the backend application using Docker. This ensures consistency between development and production environments. In MVP, we might run a single container instance, but as we progress, containers allow easy horizontal scaling (spinning up more containers for more traffic) and orchestrating using Kubernetes or similar if needed. By Phase 2 or 3, we likely transition to a Kubernetes cluster for fine-grained control and auto-scaling (for example, setting the deployment to always maintain certain performance metrics by adding pods during high load).

**Continuous Integration/Continuous Deployment (CI/CD):** We will set up a CI/CD pipeline (using tools like GitHub Actions, Jenkins, or GitLab CI). This pipeline will automatically run tests (unit and integration tests) on each code commit. If tests pass, it can automatically deploy to a staging environment. For production releases, we might require a manual approval step at MVP stage (to be cautious), but aim for an automated deploy process to minimize downtime. Each release will be versioned, and we’ll maintain a changelog.

**Staging and Production Environments:** We will have at least two environments – **Staging (Testing)** and **Production**. Staging will mirror the production environment setup but use test data. It’s where QA and user acceptance testing will happen. Only when a build is verified on staging will it be promoted to Production. This separation prevents incomplete or buggy features from affecting real users. We may also have a separate **Development** environment for internal use, but staging is critical for final checks.

**Web Deployment:** The web front-end will be built and deployed as a static bundle (if SPA) possibly served via a CDN or from an S3 bucket \+ CloudFront (on AWS) for efficiency or hosted on Vercel (will be dictated by pricing). This ensures quick load times and scalability (CDN can cache static assets globally). The SPA will talk to the API domain which is load-balanced to backend servers. If we opt for a server-side rendered approach, we’d deploy similarly as part of the backend, but likely an SPA is fine.

**Mobile Deployment:** For mobile apps, we’ll follow the standard app store deployment process. That includes configuring profiles, certificates, etc., and going through app review for iOS. Over-the-air updates will be managed via store updates (since critical logic is mostly in the backend, many changes won’t require forcing a mobile app update). We ensure that the mobile app gracefully handles connecting to updated backend APIs (backward compatibility or version checks) to not break if the backend is ahead of the mobile version.

**Monitoring and Logging:** As soon as we deploy, we’ll set up monitoring. This includes application logs (using a centralized logging service like CloudWatch Logs, ELK stack, or Datadog). We’ll log events like user sign-ins, property assessment requests, and errors/exceptions with sufficient detail (but anonymized where needed to protect PII). We will also use uptime monitoring for the API (pinging health endpoints) and set up alerts (email/SMS to devops team) if the service goes down or if error rates spike. Performance monitoring (APM) will track response times for key endpoints to catch any bottlenecks early.

Hotjar is a very good monitoring tool for the early phase to record user sessions, identify frustration and errors. Vercel has built in logging for the website.

**Scalability in Deployment:** Initially one instance can handle MVP load, but the deployment is configured such that adding more instances is straightforward. Using an auto-scaling group (on AWS EC2 or Kubernetes HPA) we can specify triggers (CPU \> X%, or queue length, etc.) to scale out. The stateless nature of the app ensures new instances can spin up and just connect to the shared DB and operate. We also plan for using a load balancer (like AWS ALB) to distribute traffic. Session management will be either stateless (JWT) or stored in a shared in-memory store like Redis if needed, to avoid any “sticky session” requirement that complicates scaling.

Managed services such as Google Cloud Run should be considered, especially in the beginning as they offer a very good free tier. Pricing will dictate the scalability models.

**Data Backup and Migration:** The database will have automated backups daily (for RDS or using pg\_dump if needed). We’ll also implement point-in-time recovery if offered, as mistakes in tax data can be costly to lose. For file storage (if we store PDFs or uploads), we ensure the storage is redundant (like S3 which is inherently redundant across availability zones). We’ll test restoration procedures to be confident we can recover from data loss. Additionally, if we update the database schema (migrations), we’ll use migration tools and run them on staging first to catch any issues.

**Security in Deployment:** We will obtain SSL/TLS certificates and serve all traffic over HTTPS (the nature of data demands it). We’ll enforce secure communication on the API. Secrets (DB passwords, API keys) will be stored in secure config (environment variables possibly managed by a secrets manager service). Server instances will be firewalled to only allow necessary ports (443 for web API, DB not exposed publicly, etc.). We’ll also consider network segregation – maybe the DB in a private subnet inaccessible from the internet. Regular security patches for server OS or libraries will be part of deployment (CI can warn about vulnerabilities).

### **6.2 Testing Plans (Stating the obvious, but wanted to call out best practices)** 

Testing will be comprehensive across different levels – from unit tests for small components to full end-to-end scenario tests. Below is our testing approach:

* **Unit Testing:** Developers will write unit tests for all core logic. This includes tests for the **assessment calculations** (e.g., given various input scenarios, does the module return the expected viability result?). If we have rules like “10% threshold”, we’ll test borderline cases (9%, 10%, 11%) to ensure correct behavior. The PDF generation code will be unit tested by verifying that given sample input data, the fields in the PDF output are correctly filled (this might be done by reading the output PDF fields or generating a known hash). Any utility functions (like data formatters, API response parsers) will have unit tests. We aim for a high coverage particularly on the parts that might have financial implications (calculation of savings, etc.).

* **Integration Testing:** We will test how different components work together. For example, a test that simulates a user input flow: calling the API to create a property, then trigger assessment, then generate form, and verifying each stage’s output. We will use either in-memory tests or a staging database for this. Integration tests will also cover external API calls: we will use mocking for third-party APIs (to not depend on external uptime for our tests, and to not incur costs/hit rate limits during testing). For instance, we can record a typical Zillow API response and use that in tests to ensure our data parsing and viability logic handle it as expected.

* **CUJ End-to-End Testing (UI Testing):** Using tools like Selenium or Cypress for web, and Appium for mobile, we will script user flows to catch any UI-level issues. For MVP, this can be relatively straightforward: a test that opens the web app, inputs a known property address, waits for the result to display, checks that the result text matches expected (based on a stubbed backend in test or known static data), then clicks “Generate PDF” and verifies that a file was indeed downloaded (perhaps check an API call was made). Similarly for mobile, automated UI tests or at least manual testing on both iOS and Android devices will be done for key flows. Given the importance of real-time feedback, we’ll test that the loading indicator shows up if we intentionally delay the backend response, and that the UI does update when the data arrives.

* **Performance Testing:** Although MVP is not at scale, we will do some basic load testing to ensure the system can handle expected usage spikes (for example, perhaps many people file appeals just before the deadline date). We can use JMeter or Locust to simulate multiple concurrent users hitting the assessment API. We’ll measure response times and see if any degrade significantly with, say, 50 concurrent requests. This helps identify if any part of the code is too slow or if database indexing is needed. We will also test memory usage and CPU to ensure our container sizing is adequate. If any heavy computation (like generating 50 PDFs at once, or processing a large dataset) is discovered to be slow, we may decide to implement asynchronous processing or queues in a later iteration. For now, performance testing ensures no obvious bottleneck in MVP code.

* **User Acceptance Testing (UAT):** We intend to run a UAT with a small group of pilot users (possibly those same friendly homeowners or advisors). They will use the system in as real a scenario as possible: input their property, see if the feedback makes sense to them, generate the form, maybe even file it for real if timing aligns. We’ll collect their feedback on any bugs or confusion. UAT is crucial for capturing things that automated tests can’t, especially around UX. For example, maybe a user entered a dollar sign in the value field and that broke something – automated tests might not catch that if we didn’t think of it, but a human might do it. So we incorporate that feedback and then write new tests if needed (like validating input sanitization).

* **Regression Testing:** As we move through phases, we’ll maintain a suite of regression tests to ensure that new features or changes don’t break existing functionality. For instance, if in Phase 2 we introduce a new data source integration, we’ll re-run all MVP tests to ensure the base behavior remains consistent. CI helps here by re-running test suites on code changes.

* **Testing Compliance Scenarios:** We will test scenarios that involve compliance, like date validations (ensuring the system rejects or warns when filing after deadline), or user role constraints (if any differences in access). Also, if there are any legal statements or disclaimers in the UI (which there likely should be – e.g., “This is not legal advice” etc.), we ensure they are present and correct.

* **Error Recovery Testing:** Induce some failures to see system behavior – e.g., shut off the connection to third-party API in a test to confirm the system handles it gracefully (maybe uses cached data or returns an error message and not a crash). Test what happens if the database goes down (maybe the app shows an error page – we can simulate by pointing to a wrong DB in a staging environment to see our error handling). While we cannot test every failure, we want to ensure the system fails safely: no sensitive info leaked in error messages, and user is informed to retry later if something goes wrong.

* **Security Testing:** We will perform basic security tests on the MVP. This includes verifying that the API endpoints are properly authenticated (no data leaks without a token), testing role-based restrictions if any (though minimal in MVP), and checking for common vulnerabilities (SQL injection, XSS in form inputs, etc.). We can use tools like OWASP ZAP to scan the web app for vulnerabilities. If possible, a professional security review or at least a checklist (OWASP top 10\) will be done. Ensuring that for example, an authenticated user cannot access another user’s data by changing an ID in an URL (we will test such scenarios manually or with scripts).

* **Compliance Testing:** If there are compliance requirements (like data retention or format), we test those. For example, if the form requires a specific format of parcel number, we test that our output matches it. Or if the jurisdiction expects two copies of the form, we might test that (though likely user will handle copies). Also, test that any calculations (like estimated taxes) use the correct tax rate and formula.

* **Beta Testing & Iteration:** After internal testing, we might label the MVP launch as a “beta” and closely monitor it with real users. Any bugs found in production by early users will be triaged and fixed promptly with patch releases. Having monitoring (as mentioned) helps catch issues even if users don’t report them explicitly. For example, if an error is thrown on a certain input pattern and logged, we can proactively fix it.

* **Documentation and Test Cases:** We will maintain documentation of test cases and results. This includes a test plan document enumerating what we intend to test for each component, and after testing, logging that it passed or noting issues. This is useful for team knowledge and for any future audits (especially if we ever need certification or to demonstrate due diligence in quality assurance to partners/investors).

In essence, testing is woven throughout development (via CI) and pre-launch (via UAT). Our goal is to deliver a **reliable MVP** – users should not encounter crashes or incorrect calculations. By catching problems early in testing, we avoid eroding user trust later. Moreover, the testing framework we put in place now will support faster development in later phases, as we’ll have safety nets to modify and extend the code with confidence.

## **7\. Third-Party Data Sources and API Recommendations**

A critical aspect of Bver’s platform is obtaining accurate and up-to-date property data and valuations. Leveraging third-party data sources can greatly enhance our automatic assessment’s accuracy and the user experience (by reducing manual input). Below are recommendations for third-party data sources and APIs that Bver can integrate, categorized by the type of data they provide:

* **Property Valuation and AVM (Automated Valuation Model) APIs:**

  * **Zillow API (Zestimates):** Zillow provides APIs that give the **Zestimate**, which is Zillow’s automated estimate of a home’s market value, along with other valuation data like a value range and comparable recent sales. The Zillow API can also return property details (beds, baths, square footage, last sale price) which are useful for our analysis.

  * **RentCast API (RealtyMole):** RentCast (formerly RealtyMole) offers an API which provides **home value estimates (AVM)** and even suggests comparable sales, plus tax assessment history. It pulls data from public records and MLS listings.

  * **ATTOM Property Data API:** ATTOM is a comprehensive real estate data provider covering **99% of U.S. properties** with a wealth of information. Their API can return property characteristics, ownership, **current and historical tax assessments**, sales history, and even an “ATTOMized AVM” (their own valuation model).

  * **Local Government Data Feeds:** Some jurisdictions offer APIs or bulk data for assessments and sales. For instance, certain counties or cities have open data portals (often Socrata-powered) where one can query assessment rolls. **Example:** City of Albany, NY provides a dataset of property tax assessments with an API endpoint. While not all places have this, a strategy could be: for key target counties, check if open data exists.

* **Tax Assessment and Public Records Databases:**

  * **CoreLogic & Others:** CoreLogic is another major data provider (similar to ATTOM) offering property tax assessments, characteristics, etc.. Services like CoreLogic typically provide data via licensed bulk data or APIs indirectly (like through partners). ActiveProspect’s offering as seen in search results suggests integration options.

  * **MLS (Multiple Listing Service) Data:** Having access to MLS sales data can improve our comps and valuations. Some APIs (like Zillow, RentCast) already incorporate MLS-fed info for listings. There is also a National Association of Realtors API (Realtors Property Resource, RPR) which is available to Realtors and provides comprehensive property reports. If any team members or partners have Realtor credentials, RPR could be tapped for deep data (including neighborhood, school info in addition to values). *Recommendation:* If a partnership or credential allows, RPR could enhance data for users who are realtors/tax consultants. Not primary for MVP, but optional path.

  * **Geographic Information Systems (GIS) data:** Many counties have GIS data for parcels (shapefiles, etc.). While not directly needed for an appeal, such data could eventually help identify property characteristics or location-based adjustments (for example, if we know a property’s exact lot size or map location relative to boundaries, etc.). Some GIS systems have APIs or at least we could use them to verify parcel numbers. *Recommendation:* For MVP, not necessary. In future, tapping GIS could help when expanding to things like a map of comps for the user to visualize or to integrate with Google Maps to show the property and comps on a map in the UI.

* **Public Records for Evidence:**  
   Besides valuations, appeals benefit from evidence like photographs, condition reports, etc. While this is often user-provided, third-party sources can help:

  * **Building Permit and Code Violation Data:** An appeal might be strengthened if a property had a recent negative event (e.g., code violations, or a lack of permitted improvements contrary to assessor assumptions). Some cities have open APIs for permits/violations. Not in MVP, but later we could integrate such data to automatically flag if a property had a recent permit (meaning maybe improvements not yet assessed) or violations (could lower value).

  * **Economic and Neighborhood Data:** Services like ATTOM and others also provide neighborhood-level data (crime rate, school ratings, etc.). These might indirectly support appeals (for example, a user could argue high crime lowers value). Bver might incorporate these as context in the future. *Recommendation:* Nice-to-have for later; not essential in MVP automated module, but including a summary of neighborhood trends in a report feature could differentiate us later.

* **APIs for Submission/Forms:**  
   While currently appeals are filed via PDF, if any jurisdictions open up e-filing, we would integrate those. For instance, some counties might have a web portal; if they had an API (rare, but maybe in some modern counties or via partners like Tyler Technologies), we would use it. Tyler Technologies (provider of many local government systems) has a “Virtual Appeals” platform – if accessible, an integration could allow direct filing. *Recommendation:* Monitor if any standardized submission APIs emerge, perhaps via government partnerships. Not applicable in MVP, but on the radar for future to truly close the loop in the workflow.

* **General Tech APIs:**

  * **Google Maps Geocoding:** to validate and standardize addresses. This reduces user error in addresses and ensures we use correct jurisdiction. (E.g., input “123 Main St” could correspond to multiple cities, geocoding clarifies which and gives county FIPS which we can use to get correct tax data).

  * **Email/Communication APIs:** (as mentioned in integrations) – e.g., SendGrid/Resend for emails (we will integrate to send confirmation emails, etc.), Twilio for SMS if needed (deadline reminders).

**Data Accuracy and Verification:** We should use multiple sources to cross-verify critical data. For example, the official assessed value should come from an official source (user input from their bill, or a county record API). If we also pull Zillow/ATTOM data, we can alert if there’s a discrepancy (ensuring we protest the correct value). Similarly, if different AVMs (Zillow vs ATTOM) differ widely, that should be flagged for a human to review (until we build enough trust on LLM based pipeline) – but likely we’ll choose one as primary for consistency in output.

**Costs and Licensing:** We must consider the legal use of each API. Zillow’s terms for example allow free use but not for resale of data. Since we are providing a service on top of it, we’ll ensure compliance by possibly attributing data where required or obtaining permission if needed for commercial use. For ATTOM and others, budget for data will be part of our operational costs as we scale (we saw \~$850+ per month for robust data; initially we can limit calls or only upgrade when user volume justifies it). Public records are generally free, but some states might charge for bulk data. If needed, we could use FOIA/public records requests to obtain certain data dumps (like sales data) for training our model or seeding our database.

**Conclusion of Data Sources:** For MVP, **Zillow API and a limited use of RentCast or ATTOM (trial)** will likely suffice to demonstrate automatic valuation. Coupled with user-provided data and perhaps one local open dataset, we’ll have a credible automatic assessment. As Bver grows, shifting to a **paid, comprehensive API (ATTOM)** plus any available **public data** will ensure our system remains accurate and maybe even becomes a source of unique data (we might compile our own database of appeals over time). Thus, the strategy is to start lean with free data and progressively integrate richer sources as the need and budget allow.

## **8\. Compliance and Regulatory Considerations**

Operating a property tax appeal service involves navigating various legal, regulatory, and compliance domains. Bver must ensure that its platform adheres to relevant laws (both those governing the property tax appeal process itself and broader data protection laws), and that users’ actions on the platform remain within legal bounds. Below are key compliance and regulatory factors to consider:

* **Property Tax Appeal Regulations (Jurisdictional):** Each state (and often each county) has specific regulations for assessment appeals. These include **filing deadlines, required forms, evidence rules, and who is allowed to file or represent the property owner**. Bver’s platform must incorporate these rules to avoid guiding a user incorrectly. For example, as noted earlier, Texas requires filing protests by May 15 or within 30 days of the notice; Georgia gives 45 days after notice, etc. We must programmatically enforce deadline checks – if a user tries to generate an appeal after the window, the system should warn them that it’s too late (or prevent it). We also should ensure the **correct form** is used – many states have standardized forms (like Georgia’s PT-311A) which we will use; using the wrong form could invalidate an appeal. The platform will maintain a compliance matrix for each region it supports: including deadlines, form templates, and any fee requirements (some jurisdictions charge a filing fee above a certain property value – we’d need to inform the user).

* **Licensing Requirements for Tax Consultants:** A crucial regulatory aspect is whether providing this service constitutes “property tax consulting” that in some states requires a license. For instance, **Texas law mandates that anyone performing property tax consulting for others, for compensation, must be registered and licensed**. This means if Bver directly assists Texas property owners in appeals (especially if charging a fee or success fee), Bver or its agents may need to hold a Property Tax Consultant license in Texas. Similarly, other states may have restrictions on who can represent taxpayers (e.g., some allow only the owner, attorneys, or licensed agents to appear in front of boards). 

* **Unauthorized Practice of Law:** While appealing a property assessment is generally an administrative process (not necessarily requiring an attorney), we must be cautious that our guidance doesn’t stray into legal advice territory, especially if a case escalates beyond the administrative board (in some states, after administrative appeal, one can file in court – that definitely is legal representation territory). We should include a **legal disclaimer** prominently: e.g., “Information provided by Bver is for informational purposes only and does not constitute legal or tax advice. Bver is a software platform, and use of the platform does not create an attorney-client relationship.” This helps ensure users understand the limits. If a situation is complex (like a major commercial appeal or legal question), the platform might recommend seeking professional counsel.

* **Data Privacy and Protection:** Bver will handle personal data such as names, addresses, possibly mortgage amounts or financial info if users input it. We must comply with data protection laws. In the U.S., at a federal level there’s not one omnibus law like GDPR, but states have privacy laws (e.g., California’s CCPA/CPRA since our user is in California, we should consider that). We should have a clear **Privacy Policy** informing users what data we collect and how it’s used. If we use or share their data (for example, pulling their property info from a third party), we need to disclose that. Under CCPA, California users have rights to see and delete their data – we should be prepared to handle such requests (even if manually at first). Security measures (encryption, etc.) are part of compliance – as noted, we encrypt sensitive data and use HTTPS. If any sensitive personal identifiable information (PII) is stored, ensure it’s minimal and protected. For instance, property tax data is often public record (assessments are public), but something like an email or phone number we collect is private and must be safeguarded.

* **Data Usage Rights:** When integrating third-party APIs, we must adhere to their terms. For example, Zillow’s terms might forbid storing their data long-term or displaying it without attribution. We should ensure our use either qualifies under “fair use” or we get the proper license. If we use government open data, check if there are any use restrictions or required attributions. For instance, some open datasets require citing the source or have non-commercial clauses. For the ones we plan to use (like Albany’s or others), usually they are open license, but we’ll verify. If we assemble a database from public records, it’s likely fine (public domain), but if we use a proprietary source like ATTOM, that data likely cannot be resold or exposed directly to users outside our service’s context. Our compliance approach: carefully review API licenses, possibly consult legal advice for any grey areas.

* **Record Retention and Compliance:** We need to consider how long to retain user data and documents. Some industries require certain retention. While not strictly mandated for us, it might be good practice to keep copies of filed appeals for a certain period in case of disputes. Privacy considerations might lean towards not keeping data longer than necessary. We will likely allow users to delete their data if they choose (post-appeal or account deletion).

* **Compliance with Government Filing Rules:** Because we file appeals on behalf of users electronically, we must ensure our submissions are accepted. That may involve compliance tests or certifications with those government systems. Even for generating PDFs, we need to ensure our forms are identical to the official forms. For example, some counties specify that no modifications to the form layout are allowed (to be machine-readable). We should ensure our generated form is essentially a replica of the official one with just fields filled – not an altered design. We should test that a form printed from our system is accepted just like one the county provides.

* **Accessibility and Non-Discrimination Compliance:** As a public-facing platform, especially if any government partnership arises, we might need to comply with accessibility standards (Section 508 in the U.S. for government-related software, or ADA compliance for public websites). We are already aiming to be accessible in UX (discussed earlier). We should also ensure that we do not inadvertently discriminate or filter users (not likely applicable, but for instance, if we expand to various states, we offer equal service regardless of location beyond our chosen markets, etc.).

* **Tax Law Changes and Updates:** Property tax laws can change (e.g., a county could change its appeal deadline or process by law). We need a process to stay updated on such regulatory changes. This might mean assigning someone (or the product team) to annually review the rules of areas we serve, or subscribing to newsletters from tax authorities. Avalara’s content suggests staying up to date with changing regulations is important. Compliance here means the platform’s guidance must always reflect current law – giving outdated advice could harm users and our credibility. We can mitigate this by building a system to easily update deadlines/forms each year (maybe a backend admin interface or at least configuration files).

* **Terms of Service & Liability:** We will draft Terms of Service that users must agree to, which will include limitations of liability. Since outcomes can’t be guaranteed, we likely will state that we do not guarantee any particular result and are not liable for any losses if an appeal is unsuccessful or if deadlines are missed. Especially if our system were to ever mis-calculate or have downtime that caused a user to miss an appeal, we want to legally protect the company (while of course aiming never to let that happen via good design). We should include an arbitration clause or dispute resolution mechanism given this deals with potential money matters. This is not a direct regulation, but a best practice legally.

* **Success Fee Compliance:** Some jurisdictions might have regulations on contingency fees for tax consultants. For example, some states permit contingent fee arrangements for property tax consultants, others might have caps. We should verify this. In Illinois for instance, certain appeal forms ask if the filer is being compensated via contingency.

In summary, compliance for Bver spans ensuring **legal operation** in the tax advisory space and **protecting user data/privacy**. We will engage with legal counsel especially when entering highly regulated markets (like Texas consulting or if expanding to representation services) to make sure we’re on the right side of regulations. By building compliance into the design (through deadline checks, license considerations, security measures), we not only avoid legal pitfalls but also create a trustworthy platform for users and partners (like government entities or data providers) to engage with.